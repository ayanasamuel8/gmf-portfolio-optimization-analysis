# ğŸ“ˆ GMF Time Series Forecasting & Portfolio Optimization

![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)![License](https://img.shields.io/badge/License-MIT-yellow.svg)

An end-to-end quantitative analysis project to enhance portfolio management strategies for Guide Me in Finance (GMF) Investments. This repository documents the process of fetching financial data, developing predictive time series models, constructing an optimal investment portfolio based on Modern Portfolio Theory (MPT), and validating the strategy through a historical backtest.

---

## ğŸ¯ Project Overview

As a Financial Analyst at GMF Investments, the objective is to leverage data-driven insights to optimize client portfolios. This project utilizes historical financial data for three key assets with distinct risk profiles:

*   **Tesla (TSLA):** High-growth, high-volatility stock.
*   **S&P 500 ETF (SPY):** Diversified, moderate-risk market benchmark.
*   **Vanguard Total Bond Market ETF (BND):** Stable, low-risk bond fund.

By forecasting future trends for a key volatile asset and understanding asset correlations, we built and validated a portfolio that maximizes risk-adjusted returns, grounded in proven financial theories. The final model-driven strategy proved to be more efficient and less volatile than a traditional benchmark portfolio.

---

## âœ¨ Key Features

*   ğŸ“¦ **Automated Data Ingestion** â€“ Fetches over a decade of financial data using the `yfinance` API.
*   ğŸ“Š **In-Depth EDA** â€“ Analysis of price trends, volatility, return distributions, and risk metrics.
*   ğŸ¤– **Advanced Time Series Forecasting** â€“ A comparative analysis between a classical ARIMA model and a deep learning LSTM model to generate a forward-looking market view.
*   âš™ï¸ **Portfolio Optimization (MPT)** â€“ Calculation of the Efficient Frontier and identification of the Maximum Sharpe Ratio portfolio based on the model's forecast.
*   ğŸ§ª **Strategy Backtesting** â€“ Validation of the optimized portfolio's performance against a standard industry benchmark, confirming superior risk-adjusted returns.

---

## ğŸš€ Project Roadmap & Key Findings

### âœ… Completed: Task 1 â€“ Data Preprocessing & Exploratory Data Analysis

*   Successfully ingested, cleaned, and processed ten years of historical data (July 2015 â€“ July 2025) for TSLA, SPY, and BND.
*   Conducted in-depth EDA to understand the characteristics of each asset, identifying TSLA's significant volatility compared to the stability of SPY and BND.

### âœ… Completed: Task 2 â€“ Developing Time Series Forecasting Models

*   Built and compared a classical ARIMA model with a deep learning LSTM model to forecast Tesla's stock price.
*   **Key Finding:** The LSTM model vastly outperformed the ARIMA model, demonstrating its superior ability to capture the non-linear dynamics of a volatile asset.

### âœ… Completed: Task 3 â€“ Forecasting Future Market Trends

*   Utilized the superior LSTM model to generate a 12-month forward-looking forecast for TSLA.
*   **Key Finding:** The model projects a significant **bearish trend for TSLA**, with an expected annualized return of **-40.08%**. This became the central hypothesis for our optimization.

### âœ… Completed: Task 4 â€“ Portfolio Optimization

*   Combined the LSTM's bearish forecast for TSLA with the historical returns of SPY and BND.
*   Generated the Efficient Frontier using Modern Portfolio Theory, which visually confirmed that including TSLA would be inefficient.
*   **Key Finding:** The optimal **Maximum Sharpe Ratio Portfolio** recommends a highly defensive allocation:
    *   **TSLA: 0.0%**
    *   **SPY: 21.8%**
    *   **BND: 78.2%**

### âœ… Completed: Task 5 â€“ Strategy Backtesting & Validation

*   Conducted a one-year backtest of our recommended portfolio against a passive 60% SPY / 40% BND benchmark.
*   **Key Finding:** The model-driven strategy was **more efficient and less volatile**, achieving a superior risk-adjusted return.

---

## ğŸ† Final Results & Validation

The one-year backtest confirmed the value of our data-driven approach. While the benchmark portfolio had a higher absolute return, our strategy delivered a **better return for each unit of risk taken.**

| Portfolio | Total Return | **Sharpe Ratio (Risk-Adjusted Return)** |
| :--- | :--- | :--- |
| **Our Strategy** | 6.52% | **1.07** |
| **Benchmark (60/40)** | 12.47% | **1.02** |

The higher Sharpe Ratio demonstrates that our strategy successfully translated the model's forecast into a more stable and efficient portfolio, fulfilling the core project objective.

---

## ğŸ—‚ Project Structure

```
gmf-portfolio-optimization-analysis/
â”‚
â”œâ”€â”€ .github/              # GitHub Actions for CI/CD
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/        # Processed data, forecasts
â”‚   â””â”€â”€ raw/              # Raw data from yfinance
â”‚
â”œâ”€â”€ models/               # Saved trained models
â”‚
â”œâ”€â”€ notebooks/            # Jupyter Notebooks for analysis
â”‚   â”œâ”€â”€ 1.0-data_exploration.ipynb
â”‚   â”œâ”€â”€ 2.0-forecasting_models.ipynb
â”‚   â”œâ”€â”€ 3.0-forecast_future_market_trends.ipynb
â”‚   â”œâ”€â”€ 4.0-optimize_portfolio_based_on_forecast.ipynb
â”‚   â””â”€â”€ 5.0-strategy_backtesting.ipynb
â”‚
â”œâ”€â”€ reports/              # Final reports & figures
â”‚   â””â”€â”€ figures/
â”‚
â”œâ”€â”€ src/                  # Source code
â”‚   â”œâ”€â”€ data_ingestion.py # Script to fetch data
â”‚   â””â”€â”€ utils.py          # Helper functions
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ›  Setup & Installation

1.  **Clone Repository**
    ```bash
    git clone https://github.com/your-username/gmf-portfolio-optimization-analysis.git
    cd gmf-portfolio-optimization-analysis
    ```
2.  **Create Virtual Environment**

    **Windows:**
    ```bash
    python -m venv venv
    .\venv\Scripts\Activate
    ```
    **macOS / Linux:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
3.  **Install Dependencies**
    ```bash
    pip install -r requirements.txt
    ```

---

## ğŸš€ How to Use

**Step 1 â€“ Data Ingestion:**

```bash
python src/data_ingestion.py
```

This populates the `data/raw/` directory with the necessary CSV files.

**Step 2 â€“ Run Analysis Notebooks:**

Open Jupyter Lab or Jupyter Notebook:

```bash
jupyter lab
```

Run the notebooks in the `notebooks/` directory in sequential order to reproduce the entire analysis from start to finish.

---

## ğŸ’» Technologies Used

*   **Python 3.10+**
*   **Libraries:**
    *   Data & Analysis: `pandas`, `numpy`, `yfinance`
    *   Visualization: `matplotlib`, `seaborn`
    *   Time Series & ML: `statsmodels`, `tensorflow`, `scikit-learn`
    *   Portfolio Optimization: `PyPortfolioOpt`